{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4642a665",
   "metadata": {},
   "source": [
    "# C++ Code to AST Dataset Generation for CodeBERT\n",
    "\n",
    "**Objective**: Convert all C++ files from the Plagiarism Dataset into Abstract Syntax Trees (AST) for CodeBERT fine-tuning.\n",
    "\n",
    "**Input**: Programming Homework Dataset for Plagiarism Detection\n",
    "**Output**: Structured AST dataset ready for machine learning applications\n",
    "\n",
    "## Process Overview:\n",
    "1. Environment setup and library imports\n",
    "2. Dataset analysis and C++ file collection\n",
    "3. AST parser implementation\n",
    "4. Batch processing system\n",
    "5. Dataset generation and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5fda20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup completed\n",
      "Working directory: /Users/onis2/NLP/TestVersion\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Library Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# AST parsing libraries\n",
    "from pycparser import c_parser, c_ast\n",
    "from pycparser.plyparser import ParseError\n",
    "\n",
    "print(\"Environment setup completed\")\n",
    "print(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbab2b",
   "metadata": {},
   "source": [
    "## Dataset Configuration and Analysis\n",
    "\n",
    "Define dataset paths and analyze the structure to identify all C++ files for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "055773ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset structure...\n",
      "Dataset Analysis Results:\n",
      "Total courses: 4\n",
      "Total C++ files: 23586\n",
      "\n",
      "Files per course:\n",
      "  A2016: 0 files\n",
      "  A2017: 0 files\n",
      "  B2016: 12,196 files\n",
      "  B2017: 11,390 files\n",
      "\n",
      "File inventory saved to: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/cpp_files_inventory.json\n",
      "\n",
      "File inventory saved to: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/cpp_files_inventory.json\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configuration\n",
    "DATASET_ROOT = Path(\"/Users/onis2/Downloads/Plagiarism Dataset\")\n",
    "SRC_PATH = DATASET_ROOT / \"src\"\n",
    "OUTPUT_DIR = Path(\"/Users/onis2/NLP/TestVersion/cpp_ast_dataset\")\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "class DatasetAnalyzer:\n",
    "    \"\"\"Analyze dataset structure and collect C++ files.\"\"\"\n",
    "    \n",
    "    def __init__(self, src_path: Path):\n",
    "        self.src_path = src_path\n",
    "        self.courses = []\n",
    "        self.cpp_files = []\n",
    "        \n",
    "    def analyze_structure(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze dataset structure and collect statistics.\"\"\"\n",
    "        print(\"Analyzing dataset structure...\")\n",
    "        \n",
    "        # Get all courses\n",
    "        self.courses = sorted([d.name for d in self.src_path.iterdir() if d.is_dir()])\n",
    "        \n",
    "        # Collect all C++ files\n",
    "        cpp_count = 0\n",
    "        course_stats = {}\n",
    "        \n",
    "        for course in self.courses:\n",
    "            course_path = self.src_path / course\n",
    "            course_cpp_files = []\n",
    "            \n",
    "            for assignment_folder in course_path.iterdir():\n",
    "                if not assignment_folder.is_dir() or not assignment_folder.name.startswith('Z'):\n",
    "                    continue\n",
    "                    \n",
    "                for sub_assignment in assignment_folder.iterdir():\n",
    "                    if not sub_assignment.is_dir():\n",
    "                        continue\n",
    "                    \n",
    "                    # Find all .cpp files\n",
    "                    cpp_files_in_assignment = list(sub_assignment.glob(\"*.cpp\"))\n",
    "                    course_cpp_files.extend(cpp_files_in_assignment)\n",
    "                    \n",
    "                    for cpp_file in cpp_files_in_assignment:\n",
    "                        file_info = {\n",
    "                            'path': cpp_file,\n",
    "                            'course': course,\n",
    "                            'assignment': f\"{assignment_folder.name}/{sub_assignment.name}\",\n",
    "                            'student_id': cpp_file.stem,\n",
    "                            'relative_path': str(cpp_file.relative_to(self.src_path))\n",
    "                        }\n",
    "                        self.cpp_files.append(file_info)\n",
    "            \n",
    "            course_stats[course] = len(course_cpp_files)\n",
    "            cpp_count += len(course_cpp_files)\n",
    "        \n",
    "        stats = {\n",
    "            'total_courses': len(self.courses),\n",
    "            'courses': self.courses,\n",
    "            'total_cpp_files': cpp_count,\n",
    "            'files_per_course': course_stats,\n",
    "            'cpp_files': self.cpp_files\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Initialize analyzer and collect C++ files\n",
    "analyzer = DatasetAnalyzer(SRC_PATH)\n",
    "dataset_stats = analyzer.analyze_structure()\n",
    "\n",
    "print(\"Dataset Analysis Results:\")\n",
    "print(f\"Total courses: {dataset_stats['total_courses']}\")\n",
    "print(f\"Total C++ files: {dataset_stats['total_cpp_files']}\")\n",
    "print(\"\\nFiles per course:\")\n",
    "for course, count in dataset_stats['files_per_course'].items():\n",
    "    print(f\"  {course}: {count:,} files\")\n",
    "\n",
    "# Save file list for reference\n",
    "cpp_files_list = [\n",
    "    {\n",
    "        'course': f['course'],\n",
    "        'assignment': f['assignment'], \n",
    "        'student_id': f['student_id'],\n",
    "        'path': str(f['path'])\n",
    "    } \n",
    "    for f in dataset_stats['cpp_files']\n",
    "]\n",
    "\n",
    "with open(OUTPUT_DIR / \"cpp_files_inventory.json\", 'w') as f:\n",
    "    json.dump(cpp_files_list, f, indent=2)\n",
    "\n",
    "print(f\"\\nFile inventory saved to: {OUTPUT_DIR / 'cpp_files_inventory.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0372037",
   "metadata": {},
   "source": [
    "## AST Node and Parser Implementation\n",
    "\n",
    "Core classes for AST representation and C++ code parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5980e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ AST Parser initialized\n"
     ]
    }
   ],
   "source": [
    "class ASTNode:\n",
    "    \"\"\"Represents a node in the Abstract Syntax Tree.\"\"\"\n",
    "    \n",
    "    def __init__(self, node_type: str, value: Optional[str] = None, children: Optional[List['ASTNode']] = None):\n",
    "        self.node_type = node_type\n",
    "        self.value = value\n",
    "        self.children = children or []\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert AST node to dictionary representation.\"\"\"\n",
    "        return {\n",
    "            'type': self.node_type,\n",
    "            'value': self.value,\n",
    "            'children': [child.to_dict() for child in self.children]\n",
    "        }\n",
    "    \n",
    "    def to_sequence(self) -> List[str]:\n",
    "        \"\"\"Convert AST to flat sequence representation for CodeBERT.\"\"\"\n",
    "        sequence = [f\"<{self.node_type}>\"]\n",
    "        if self.value:\n",
    "            sequence.append(str(self.value))\n",
    "        \n",
    "        for child in self.children:\n",
    "            sequence.extend(child.to_sequence())\n",
    "        \n",
    "        sequence.append(f\"</{self.node_type}>\")\n",
    "        return sequence\n",
    "    \n",
    "    def extract_features(self) -> Dict[str, Any]:\n",
    "        \"\"\"Extract structural features from AST.\"\"\"\n",
    "        features = {\n",
    "            'total_nodes': 0,\n",
    "            'node_types': defaultdict(int),\n",
    "            'max_depth': 0,\n",
    "            'identifiers': set(),\n",
    "            'literals': set(),\n",
    "            'operators': set()\n",
    "        }\n",
    "        \n",
    "        def traverse(node: 'ASTNode', depth: int = 0):\n",
    "            features['total_nodes'] += 1\n",
    "            features['node_types'][node.node_type] += 1\n",
    "            features['max_depth'] = max(features['max_depth'], depth)\n",
    "            \n",
    "            if node.value:\n",
    "                if node.node_type in ['ID', 'Identifier']:\n",
    "                    features['identifiers'].add(node.value)\n",
    "                elif node.node_type in ['Constant', 'literal']:\n",
    "                    features['literals'].add(node.value)\n",
    "                elif node.node_type in ['BinaryOp', 'UnaryOp', 'Assignment']:\n",
    "                    features['operators'].add(node.value)\n",
    "            \n",
    "            for child in node.children:\n",
    "                traverse(child, depth + 1)\n",
    "        \n",
    "        traverse(self)\n",
    "        \n",
    "        # Convert sets to lists for JSON serialization\n",
    "        features['identifiers'] = list(features['identifiers'])\n",
    "        features['literals'] = list(features['literals'])\n",
    "        features['operators'] = list(features['operators'])\n",
    "        features['node_types'] = dict(features['node_types'])\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "class CppASTParser:\n",
    "    \"\"\"Enhanced C++ AST Parser with preprocessing capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parser = c_parser.CParser()\n",
    "        self.preprocessing_stats = {'successful': 0, 'failed': 0}\n",
    "    \n",
    "    def preprocess_cpp_code(self, code: str) -> str:\n",
    "        \"\"\"Preprocess C++ code to handle includes and common constructs.\"\"\"\n",
    "        # Remove includes and preprocessor directives\n",
    "        processed_code = re.sub(r'#include\\s*[<\"][^>\"]*[>\"]', '', code)\n",
    "        processed_code = re.sub(r'#ifndef.*?#endif', '', processed_code, flags=re.DOTALL)\n",
    "        processed_code = re.sub(r'#define.*?\\n', '', processed_code)\n",
    "        processed_code = re.sub(r'#pragma.*?\\n', '', processed_code)\n",
    "        \n",
    "        # Add basic type definitions and function declarations\n",
    "        declarations = '''\n",
    "typedef long size_t;\n",
    "typedef struct FILE FILE;\n",
    "extern FILE *stdin, *stdout, *stderr;\n",
    "int printf(const char *format, ...);\n",
    "int scanf(const char *format, ...);\n",
    "void *malloc(size_t size);\n",
    "void free(void *ptr);\n",
    "int strcmp(const char *s1, const char *s2);\n",
    "size_t strlen(const char *s);\n",
    "        '''\n",
    "        \n",
    "        return declarations + processed_code\n",
    "    \n",
    "    def parse_code(self, code: str, filename: str = \"<string>\") -> Optional[ASTNode]:\n",
    "        \"\"\"Parse C++ code and return AST representation.\"\"\"\n",
    "        try:\n",
    "            processed_code = self.preprocess_cpp_code(code)\n",
    "            ast = self.parser.parse(processed_code, filename=filename)\n",
    "            return self._convert_pycparser_ast(ast)\n",
    "        except ParseError as e:\n",
    "            self.preprocessing_stats['failed'] += 1\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.preprocessing_stats['failed'] += 1\n",
    "            return None\n",
    "    \n",
    "    def _convert_pycparser_ast(self, node) -> Optional[ASTNode]:\n",
    "        \"\"\"Convert pycparser AST to custom ASTNode format.\"\"\"\n",
    "        if node is None:\n",
    "            return None\n",
    "        \n",
    "        node_type = node.__class__.__name__\n",
    "        \n",
    "        # Extract node value\n",
    "        value = None\n",
    "        if hasattr(node, 'name') and node.name:\n",
    "            value = node.name\n",
    "        elif hasattr(node, 'value') and node.value:\n",
    "            value = node.value\n",
    "        elif hasattr(node, 'op') and node.op:\n",
    "            value = node.op\n",
    "        \n",
    "        # Convert children\n",
    "        children = []\n",
    "        for attr_name, attr_value in node.children():\n",
    "            if attr_value:\n",
    "                if isinstance(attr_value, list):\n",
    "                    for item in attr_value:\n",
    "                        converted_child = self._convert_pycparser_ast(item)\n",
    "                        if converted_child:\n",
    "                            children.append(converted_child)\n",
    "                else:\n",
    "                    converted_child = self._convert_pycparser_ast(attr_value)\n",
    "                    if converted_child:\n",
    "                        children.append(converted_child)\n",
    "        \n",
    "        return ASTNode(node_type, value, children)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get preprocessing statistics.\"\"\"\n",
    "        return self.preprocessing_stats.copy()\n",
    "\n",
    "\n",
    "# Initialize parser\n",
    "cpp_parser = CppASTParser()\n",
    "print(\"C++ AST Parser initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5b703",
   "metadata": {},
   "source": [
    "## Batch Processing System\n",
    "\n",
    "High-performance batch processor for converting all C++ files to AST representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c62fc4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ AST Processor initialized\n"
     ]
    }
   ],
   "source": [
    "class CppASTProcessor:\n",
    "    \"\"\"Batch processor for converting C++ files to AST representations.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.parser = CppASTParser()\n",
    "        self.processing_stats = {\n",
    "            'total_files': 0,\n",
    "            'successful_parses': 0,\n",
    "            'failed_parses': 0,\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'processing_times': [],\n",
    "            'file_sizes': [],\n",
    "            'ast_sizes': []\n",
    "        }\n",
    "    \n",
    "    def process_file(self, file_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Process a single C++ file and return AST data.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            file_path = Path(file_info['path'])\n",
    "            \n",
    "            # Read source code\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                source_code = f.read()\n",
    "            \n",
    "            # Parse to AST\n",
    "            ast_root = self.parser.parse_code(source_code, str(file_path))\n",
    "            \n",
    "            if ast_root is None:\n",
    "                return None\n",
    "            \n",
    "            # Extract features and sequence\n",
    "            ast_features = ast_root.extract_features()\n",
    "            ast_sequence = ast_root.to_sequence()\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            result = {\n",
    "                'file_info': {\n",
    "                    'course': file_info['course'],\n",
    "                    'assignment': file_info['assignment'],\n",
    "                    'student_id': file_info['student_id'],\n",
    "                    'relative_path': file_info['relative_path']\n",
    "                },\n",
    "                'source_code': source_code,\n",
    "                'ast_features': ast_features,\n",
    "                'ast_sequence': ast_sequence,\n",
    "                'processing_time': processing_time,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Update statistics\n",
    "            self.processing_stats['file_sizes'].append(len(source_code))\n",
    "            self.processing_stats['ast_sizes'].append(len(ast_sequence))\n",
    "            self.processing_stats['processing_times'].append(processing_time)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_info['relative_path']}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def process_batch(self, cpp_files: List[Dict[str, Any]], batch_size: int = 1000) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a batch of C++ files.\"\"\"\n",
    "        print(f\"Starting batch processing of {len(cpp_files)} C++ files...\")\n",
    "        \n",
    "        self.processing_stats['total_files'] = len(cpp_files)\n",
    "        self.processing_stats['start_time'] = datetime.now()\n",
    "        \n",
    "        results = []\n",
    "        failed_files = []\n",
    "        \n",
    "        # Process files with progress bar\n",
    "        for file_info in tqdm(cpp_files, desc=\"Processing C++ files\"):\n",
    "            result = self.process_file(file_info)\n",
    "            \n",
    "            if result:\n",
    "                results.append(result)\n",
    "                self.processing_stats['successful_parses'] += 1\n",
    "            else:\n",
    "                failed_files.append(file_info)\n",
    "                self.processing_stats['failed_parses'] += 1\n",
    "        \n",
    "        self.processing_stats['end_time'] = datetime.now()\n",
    "        \n",
    "        # Save results in batches to manage memory\n",
    "        self._save_results(results, failed_files)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _save_results(self, results: List[Dict[str, Any]], failed_files: List[Dict[str, Any]]):\n",
    "        \"\"\"Save processing results to files.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save successful results\n",
    "        results_file = self.output_dir / f\"cpp_ast_dataset_{timestamp}.pkl\"\n",
    "        with open(results_file, 'wb') as f:\n",
    "            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'total_files': len(results),\n",
    "            'processing_stats': self.processing_stats,\n",
    "            'parser_stats': self.parser.get_stats(),\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        metadata_file = self.output_dir / f\"metadata_{timestamp}.json\"\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            # Convert datetime objects to strings for JSON serialization\n",
    "            metadata_copy = metadata.copy()\n",
    "            if metadata_copy['processing_stats']['start_time']:\n",
    "                metadata_copy['processing_stats']['start_time'] = str(metadata_copy['processing_stats']['start_time'])\n",
    "            if metadata_copy['processing_stats']['end_time']:\n",
    "                metadata_copy['processing_stats']['end_time'] = str(metadata_copy['processing_stats']['end_time'])\n",
    "            json.dump(metadata_copy, f, indent=2)\n",
    "        \n",
    "        # Save sample data for inspection\n",
    "        sample_size = min(5, len(results))\n",
    "        if sample_size > 0:\n",
    "            sample_data = []\n",
    "            for result in results[:sample_size]:\n",
    "                sample = {\n",
    "                    'file_info': result['file_info'],\n",
    "                    'ast_features': result['ast_features'],\n",
    "                    'ast_sequence_length': len(result['ast_sequence']),\n",
    "                    'ast_sequence_sample': result['ast_sequence'][:20],\n",
    "                    'processing_time': result['processing_time']\n",
    "                }\n",
    "                sample_data.append(sample)\n",
    "            \n",
    "            sample_file = self.output_dir / f\"sample_results_{timestamp}.json\"\n",
    "            with open(sample_file, 'w') as f:\n",
    "                json.dump(sample_data, f, indent=2)\n",
    "        \n",
    "        # Save failed files list\n",
    "        if failed_files:\n",
    "            failed_file = self.output_dir / f\"failed_files_{timestamp}.txt\"\n",
    "            with open(failed_file, 'w') as f:\n",
    "                for file_info in failed_files:\n",
    "                    f.write(f\"{file_info['relative_path']}\\\\n\")\n",
    "        \n",
    "        print(f\"Results saved:\")\n",
    "        print(f\"  Main dataset: {results_file}\")\n",
    "        print(f\"  Metadata: {metadata_file}\")\n",
    "        if sample_size > 0:\n",
    "            print(f\"  Sample data: {sample_file}\")\n",
    "        if failed_files:\n",
    "            print(f\"  Failed files: {failed_file}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print processing summary.\"\"\"\n",
    "        stats = self.processing_stats\n",
    "        \n",
    "        # Handle duration calculation properly\n",
    "        duration = None\n",
    "        if stats['end_time'] and stats['start_time']:\n",
    "            if isinstance(stats['end_time'], str):\n",
    "                # Convert string back to datetime for calculation\n",
    "                from datetime import datetime\n",
    "                try:\n",
    "                    end_time = datetime.fromisoformat(stats['end_time'].replace('Z', '+00:00'))\n",
    "                    start_time = datetime.fromisoformat(stats['start_time'].replace('Z', '+00:00'))\n",
    "                    duration = end_time - start_time\n",
    "                except:\n",
    "                    duration = None\n",
    "            else:\n",
    "                # Already datetime objects\n",
    "                duration = stats['end_time'] - stats['start_time']\n",
    "        \n",
    "        print(\"\\\\nProcessing Summary:\")\n",
    "        print(f\"Total files processed: {stats['total_files']}\")\n",
    "        print(f\"Successful parses: {stats['successful_parses']}\")\n",
    "        print(f\"Failed parses: {stats['failed_parses']}\")\n",
    "        \n",
    "        if stats['total_files'] > 0:\n",
    "            success_rate = (stats['successful_parses'] / stats['total_files']) * 100\n",
    "            print(f\"Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        if duration:\n",
    "            print(f\"Processing duration: {duration}\")\n",
    "        \n",
    "        if stats['processing_times']:\n",
    "            avg_time = np.mean(stats['processing_times'])\n",
    "            print(f\"Average processing time: {avg_time:.3f}s per file\")\n",
    "        \n",
    "        if stats['file_sizes']:\n",
    "            avg_size = np.mean(stats['file_sizes'])\n",
    "            print(f\"Average file size: {avg_size:.0f} characters\")\n",
    "        \n",
    "        if stats['ast_sizes']:\n",
    "            avg_ast_size = np.mean(stats['ast_sizes'])\n",
    "            print(f\"Average AST sequence length: {avg_ast_size:.0f} tokens\")\n",
    "\n",
    "\n",
    "# Initialize processor\n",
    "processor = CppASTProcessor(OUTPUT_DIR)\n",
    "print(\"C++ AST Processor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f458873",
   "metadata": {},
   "source": [
    "## Dataset Generation and Processing\n",
    "\n",
    "Execute the full pipeline to convert all C++ files to AST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fbaa0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting C++ to AST dataset generation...\n",
      "Processing 23586 C++ files\n",
      "Output directory: /Users/onis2/NLP/TestVersion/cpp_ast_dataset\n",
      "Starting batch processing of 23586 C++ files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C++ files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23586/23586 [00:18<00:00, 1264.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved:\n",
      "  Main dataset: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/cpp_ast_dataset_20250921_164042.pkl\n",
      "  Metadata: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/metadata_20250921_164042.json\n",
      "  Sample data: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/sample_results_20250921_164042.json\n",
      "  Failed files: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/failed_files_20250921_164042.txt\n",
      "\\nProcessing Summary:\n",
      "Total files processed: 23586\n",
      "Successful parses: 172\n",
      "Failed parses: 23414\n",
      "Success rate: 0.7%\n",
      "Processing duration: 0:00:18.653771\n",
      "Average processing time: 0.001s per file\n",
      "Average file size: 27 characters\n",
      "Average AST sequence length: 193 tokens\n",
      "\\nDataset generation completed!\n",
      "Generated AST representations for 172 C++ files\n",
      "Results saved in: /Users/onis2/NLP/TestVersion/cpp_ast_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute full dataset processing pipeline\n",
    "\n",
    "print(\"Starting C++ to AST dataset generation...\")\n",
    "print(f\"Processing {len(dataset_stats['cpp_files'])} C++ files\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Process all C++ files\n",
    "results = processor.process_batch(dataset_stats['cpp_files'])\n",
    "\n",
    "# Print processing summary\n",
    "processor.print_summary()\n",
    "\n",
    "print(f\"\\\\nDataset generation completed!\")\n",
    "print(f\"Generated AST representations for {len(results)} C++ files\")\n",
    "print(f\"Results saved in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86faa2",
   "metadata": {},
   "source": [
    "## CodeBERT Dataset Preparation\n",
    "\n",
    "Prepare the AST dataset for CodeBERT training with proper formatting and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd225c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting 172 AST results for CodeBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting for CodeBERT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:00<00:00, 305945.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeBERT dataset saved:\n",
      "  JSON format: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/codebert_cpp_dataset_20250921_164042.json\n",
      "  CSV format: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/codebert_cpp_dataset_20250921_164042.csv\n",
      "  Summary: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/dataset_summary_20250921_164042.json\n",
      "\\nDataset Summary:\n",
      "Total samples: 172\n",
      "Courses: B2016, B2017\n",
      "Average sequence length: 189.1\n",
      "Average AST nodes: 85.4\n",
      "Average AST depth: 7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class CodeBERTDatasetFormatter:\n",
    "    \"\"\"Format AST dataset for CodeBERT training.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "    \n",
    "    def format_for_codebert(self, ast_results: List[Dict[str, Any]], max_sequence_length: int = 512) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Format AST data for CodeBERT input.\"\"\"\n",
    "        formatted_data = []\n",
    "        \n",
    "        print(f\"Formatting {len(ast_results)} AST results for CodeBERT...\")\n",
    "        \n",
    "        for result in tqdm(ast_results, desc=\"Formatting for CodeBERT\"):\n",
    "            # Truncate AST sequence to fit model constraints\n",
    "            ast_sequence = result['ast_sequence'][:max_sequence_length]\n",
    "            \n",
    "            # Create CodeBERT-compatible format\n",
    "            formatted_entry = {\n",
    "                'id': f\"{result['file_info']['course']}_{result['file_info']['assignment']}_{result['file_info']['student_id']}\",\n",
    "                'text': ' '.join(ast_sequence),\n",
    "                'ast_sequence': ast_sequence,\n",
    "                'metadata': {\n",
    "                    'course': result['file_info']['course'],\n",
    "                    'assignment': result['file_info']['assignment'],\n",
    "                    'student_id': result['file_info']['student_id'],\n",
    "                    'ast_features': result['ast_features'],\n",
    "                    'original_sequence_length': len(result['ast_sequence']),\n",
    "                    'truncated': len(result['ast_sequence']) > max_sequence_length,\n",
    "                    'processing_time': result['processing_time']\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            formatted_data.append(formatted_entry)\n",
    "        \n",
    "        return formatted_data\n",
    "    \n",
    "    def save_codebert_dataset(self, formatted_data: List[Dict[str, Any]]):\n",
    "        \"\"\"Save formatted dataset for CodeBERT.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save complete dataset\n",
    "        dataset_file = self.output_dir / f\"codebert_cpp_dataset_{timestamp}.json\"\n",
    "        with open(dataset_file, 'w') as f:\n",
    "            json.dump(formatted_data, f, indent=2)\n",
    "        \n",
    "        # Create training data CSV for easy loading\n",
    "        csv_data = []\n",
    "        for entry in formatted_data:\n",
    "            csv_data.append({\n",
    "                'id': entry['id'],\n",
    "                'text': entry['text'],\n",
    "                'course': entry['metadata']['course'],\n",
    "                'assignment': entry['metadata']['assignment'],\n",
    "                'student_id': entry['metadata']['student_id'],\n",
    "                'ast_nodes': entry['metadata']['ast_features']['total_nodes'],\n",
    "                'ast_depth': entry['metadata']['ast_features']['max_depth'],\n",
    "                'sequence_length': len(entry['ast_sequence'])\n",
    "            })\n",
    "        \n",
    "        csv_file = self.output_dir / f\"codebert_cpp_dataset_{timestamp}.csv\"\n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        summary = {\n",
    "            'total_samples': len(formatted_data),\n",
    "            'courses': list(df['course'].unique()),\n",
    "            'assignments_per_course': df.groupby('course')['assignment'].nunique().to_dict(),\n",
    "            'avg_sequence_length': df['sequence_length'].mean(),\n",
    "            'sequence_length_stats': {\n",
    "                'min': int(df['sequence_length'].min()),\n",
    "                'max': int(df['sequence_length'].max()),\n",
    "                'mean': float(df['sequence_length'].mean()),\n",
    "                'std': float(df['sequence_length'].std())\n",
    "            },\n",
    "            'avg_ast_nodes': df['ast_nodes'].mean(),\n",
    "            'avg_ast_depth': df['ast_depth'].mean(),\n",
    "            'generation_timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        summary_file = self.output_dir / f\"dataset_summary_{timestamp}.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"CodeBERT dataset saved:\")\n",
    "        print(f\"  JSON format: {dataset_file}\")\n",
    "        print(f\"  CSV format: {csv_file}\")\n",
    "        print(f\"  Summary: {summary_file}\")\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Format dataset for CodeBERT (only if results were generated)\n",
    "if 'results' in locals() and results:\n",
    "    formatter = CodeBERTDatasetFormatter(OUTPUT_DIR)\n",
    "    codebert_data = formatter.format_for_codebert(results, max_sequence_length=512)\n",
    "    dataset_summary = formatter.save_codebert_dataset(codebert_data)\n",
    "    \n",
    "    print(\"\\\\nDataset Summary:\")\n",
    "    print(f\"Total samples: {dataset_summary['total_samples']}\")\n",
    "    print(f\"Courses: {', '.join(dataset_summary['courses'])}\")\n",
    "    print(f\"Average sequence length: {dataset_summary['avg_sequence_length']:.1f}\")\n",
    "    print(f\"Average AST nodes: {dataset_summary['avg_ast_nodes']:.1f}\")\n",
    "    print(f\"Average AST depth: {dataset_summary['avg_ast_depth']:.1f}\")\n",
    "else:\n",
    "    print(\"No results available for formatting. Please run the processing step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944b018",
   "metadata": {},
   "source": [
    "## Dataset Validation and Analysis\n",
    "\n",
    "Validate the generated dataset and provide comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00bb777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetValidator:\n",
    "    def __init__(self, dataset_dir):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    def validate_dataset(self):\n",
    "        \"\"\"Validate the generated dataset\"\"\"\n",
    "        print(f\"Validating dataset in: {self.dataset_dir}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Check dataset structure\n",
    "        self._validate_structure()\n",
    "        self._validate_files()\n",
    "        self._validate_ast_quality()\n",
    "        self._generate_statistics()\n",
    "        \n",
    "        return self.validation_results\n",
    "    \n",
    "    def _validate_structure(self):\n",
    "        \"\"\"Validate dataset directory structure\"\"\"\n",
    "        print(\"Validating dataset structure...\")\n",
    "        \n",
    "        required_files = ['metadata.json', 'samples.json']\n",
    "        required_dirs = ['individual_asts', 'processed_files']\n",
    "        \n",
    "        structure_valid = True\n",
    "        \n",
    "        # Check required files\n",
    "        for file in required_files:\n",
    "            file_path = os.path.join(self.dataset_dir, file)\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"‚úì {file} exists\")\n",
    "            else:\n",
    "                print(f\"‚úó {file} missing\")\n",
    "                structure_valid = False\n",
    "        \n",
    "        # Check required directories\n",
    "        for dir_name in required_dirs:\n",
    "            dir_path = os.path.join(self.dataset_dir, dir_name)\n",
    "            if os.path.exists(dir_path):\n",
    "                file_count = len(os.listdir(dir_path))\n",
    "                print(f\"‚úì {dir_name} exists ({file_count} files)\")\n",
    "            else:\n",
    "                print(f\"‚úó {dir_name} missing\")\n",
    "                structure_valid = False\n",
    "        \n",
    "        self.validation_results['structure_valid'] = structure_valid\n",
    "        print()\n",
    "    \n",
    "    def _validate_files(self):\n",
    "        \"\"\"Validate individual files\"\"\"\n",
    "        print(\"Validating individual files...\")\n",
    "        \n",
    "        metadata_path = os.path.join(self.dataset_dir, 'metadata.json')\n",
    "        samples_path = os.path.join(self.dataset_dir, 'samples.json')\n",
    "        \n",
    "        file_validation = {}\n",
    "        \n",
    "        # Validate metadata.json\n",
    "        if os.path.exists(metadata_path):\n",
    "            try:\n",
    "                with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "                    metadata = json.load(f)\n",
    "                    required_keys = ['total_files', 'successful_conversions', 'success_rate', 'created_at']\n",
    "                    \n",
    "                    metadata_valid = all(key in metadata for key in required_keys)\n",
    "                    file_validation['metadata'] = {\n",
    "                        'valid': metadata_valid,\n",
    "                        'content': metadata\n",
    "                    }\n",
    "                    print(f\"‚úì metadata.json is valid\")\n",
    "            except Exception as e:\n",
    "                file_validation['metadata'] = {'valid': False, 'error': str(e)}\n",
    "                print(f\"‚úó metadata.json validation failed: {e}\")\n",
    "        \n",
    "        # Validate samples.json\n",
    "        if os.path.exists(samples_path):\n",
    "            try:\n",
    "                with open(samples_path, 'r', encoding='utf-8') as f:\n",
    "                    samples = json.load(f)\n",
    "                    \n",
    "                    samples_valid = isinstance(samples, list) and len(samples) > 0\n",
    "                    if samples_valid and len(samples) > 0:\n",
    "                        # Check first sample structure\n",
    "                        first_sample = samples[0]\n",
    "                        required_keys = ['file_path', 'file_size', 'ast_sequence']\n",
    "                        samples_valid = all(key in first_sample for key in required_keys)\n",
    "                    \n",
    "                    file_validation['samples'] = {\n",
    "                        'valid': samples_valid,\n",
    "                        'count': len(samples) if isinstance(samples, list) else 0\n",
    "                    }\n",
    "                    print(f\"‚úì samples.json is valid ({len(samples)} samples)\")\n",
    "            except Exception as e:\n",
    "                file_validation['samples'] = {'valid': False, 'error': str(e)}\n",
    "                print(f\"‚úó samples.json validation failed: {e}\")\n",
    "        \n",
    "        self.validation_results['file_validation'] = file_validation\n",
    "        print()\n",
    "    \n",
    "    def _validate_ast_quality(self):\n",
    "        \"\"\"Validate AST quality\"\"\"\n",
    "        print(\"Validating AST quality...\")\n",
    "        \n",
    "        samples_path = os.path.join(self.dataset_dir, 'samples.json')\n",
    "        \n",
    "        if not os.path.exists(samples_path):\n",
    "            print(\"‚úó Cannot validate AST quality - samples.json not found\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            with open(samples_path, 'r', encoding='utf-8') as f:\n",
    "                samples = json.load(f)\n",
    "            \n",
    "            quality_metrics = {\n",
    "                'total_samples': len(samples),\n",
    "                'avg_sequence_length': 0,\n",
    "                'min_sequence_length': float('inf'),\n",
    "                'max_sequence_length': 0,\n",
    "                'empty_sequences': 0,\n",
    "                'valid_sequences': 0\n",
    "            }\n",
    "            \n",
    "            sequence_lengths = []\n",
    "            \n",
    "            for sample in samples:\n",
    "                if 'ast_sequence' in sample:\n",
    "                    seq_len = len(sample['ast_sequence'])\n",
    "                    sequence_lengths.append(seq_len)\n",
    "                    \n",
    "                    if seq_len == 0:\n",
    "                        quality_metrics['empty_sequences'] += 1\n",
    "                    else:\n",
    "                        quality_metrics['valid_sequences'] += 1\n",
    "                        quality_metrics['min_sequence_length'] = min(quality_metrics['min_sequence_length'], seq_len)\n",
    "                        quality_metrics['max_sequence_length'] = max(quality_metrics['max_sequence_length'], seq_len)\n",
    "            \n",
    "            if sequence_lengths:\n",
    "                quality_metrics['avg_sequence_length'] = sum(sequence_lengths) / len(sequence_lengths)\n",
    "                quality_metrics['median_sequence_length'] = sorted(sequence_lengths)[len(sequence_lengths)//2]\n",
    "            \n",
    "            if quality_metrics['min_sequence_length'] == float('inf'):\n",
    "                quality_metrics['min_sequence_length'] = 0\n",
    "            \n",
    "            print(f\"‚úì AST Quality Analysis:\")\n",
    "            print(f\"  Total samples: {quality_metrics['total_samples']}\")\n",
    "            print(f\"  Valid sequences: {quality_metrics['valid_sequences']}\")\n",
    "            print(f\"  Empty sequences: {quality_metrics['empty_sequences']}\")\n",
    "            print(f\"  Avg sequence length: {quality_metrics['avg_sequence_length']:.1f}\")\n",
    "            print(f\"  Min sequence length: {quality_metrics['min_sequence_length']}\")\n",
    "            print(f\"  Max sequence length: {quality_metrics['max_sequence_length']}\")\n",
    "            \n",
    "            self.validation_results['ast_quality'] = quality_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó AST quality validation failed: {e}\")\n",
    "            self.validation_results['ast_quality'] = {'error': str(e)}\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    def _generate_statistics(self):\n",
    "        \"\"\"Generate comprehensive statistics\"\"\"\n",
    "        print(\"Generating comprehensive statistics...\")\n",
    "        \n",
    "        try:\n",
    "            metadata_path = os.path.join(self.dataset_dir, 'metadata.json')\n",
    "            \n",
    "            if os.path.exists(metadata_path):\n",
    "                with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                print(f\"Dataset Statistics:\")\n",
    "                print(f\"  Created: {metadata.get('created_at', 'Unknown')}\")\n",
    "                print(f\"  Total C++ files processed: {metadata.get('total_files', 0):,}\")\n",
    "                print(f\"  Successful conversions: {metadata.get('successful_conversions', 0):,}\")\n",
    "                print(f\"  Success rate: {metadata.get('success_rate', 0):.1f}%\")\n",
    "                print(f\"  Failed conversions: {metadata.get('total_files', 0) - metadata.get('successful_conversions', 0):,}\")\n",
    "                \n",
    "                # Calculate dataset size\n",
    "                dataset_size = 0\n",
    "                for root, dirs, files in os.walk(self.dataset_dir):\n",
    "                    for file in files:\n",
    "                        dataset_size += os.path.getsize(os.path.join(root, file))\n",
    "                \n",
    "                print(f\"  Dataset size: {dataset_size / (1024*1024):.1f} MB\")\n",
    "                \n",
    "                self.validation_results['statistics'] = {\n",
    "                    'metadata': metadata,\n",
    "                    'dataset_size_mb': dataset_size / (1024*1024)\n",
    "                }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Statistics generation failed: {e}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    def export_validation_report(self):\n",
    "        \"\"\"Export validation report\"\"\"\n",
    "        report_path = os.path.join(self.dataset_dir, 'validation_report.json')\n",
    "        \n",
    "        try:\n",
    "            with open(report_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.validation_results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"‚úì Validation report exported to: {report_path}\")\n",
    "            return report_path\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to export validation report: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a21892",
   "metadata": {},
   "source": [
    "## Execute Complete Pipeline\n",
    "\n",
    "Run the complete pipeline to process all C++ files and generate the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a55b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting C++ AST Dataset Generation Pipeline\n",
      "============================================================\n",
      "\n",
      "üîç Step 1: Analyzing dataset...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetAnalyzer' object has no attribute 'analyze_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Execute the pipeline\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmain_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mmain_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîç Step 1: Analyzing dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m DatasetAnalyzer(plagiarism_dataset_path)\n\u001b[0;32m---> 14\u001b[0m cpp_files \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_dataset\u001b[49m()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cpp_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m C++ files for processing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cpp_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetAnalyzer' object has no attribute 'analyze_dataset'"
     ]
    }
   ],
   "source": [
    "def main_pipeline():\n",
    "    \"\"\"Execute the complete C++ AST dataset generation pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting C++ AST Dataset Generation Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Configuration\n",
    "    plagiarism_dataset_path = Path(\"/Users/onis2/Downloads/Plagiarism Dataset/src\")\n",
    "    output_dir = Path(\"/Users/onis2/NLP/TestVersion/cpp_ast_dataset\")\n",
    "    \n",
    "    # Step 1: Analyze dataset\n",
    "    print(\"\\nüîç Step 1: Analyzing dataset...\")\n",
    "    analyzer = DatasetAnalyzer(plagiarism_dataset_path)\n",
    "    dataset_stats = analyzer.analyze_structure()\n",
    "    cpp_files = dataset_stats['cpp_files']\n",
    "    \n",
    "    print(f\"Found {len(cpp_files)} C++ files for processing\")\n",
    "    \n",
    "    if len(cpp_files) == 0:\n",
    "        print(\"‚ùå No C++ files found. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Process C++ files to AST\n",
    "    print(f\"\\nüîÑ Step 2: Processing {len(cpp_files)} C++ files...\")\n",
    "    processor = CppASTProcessor(output_dir)\n",
    "    processing_results = processor.process_batch(cpp_files)\n",
    "    \n",
    "    # Step 3: Format for CodeBERT\n",
    "    print(f\"\\nüìä Step 3: Formatting dataset for CodeBERT...\")\n",
    "    formatter = CodeBERTDatasetFormatter(output_dir)\n",
    "    codebert_data = formatter.format_for_codebert(processing_results, max_sequence_length=512)\n",
    "    dataset_summary = formatter.save_codebert_dataset(codebert_data)\n",
    "    \n",
    "    # Step 4: Validate dataset\n",
    "    print(f\"\\n‚úÖ Step 4: Validating generated dataset...\")\n",
    "    validator = DatasetValidator(output_dir)\n",
    "    validation_results = validator.validate_dataset()\n",
    "    validator.export_validation_report()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"üìÅ Dataset location: {output_dir}\")\n",
    "    print(f\"üìà Total C++ files found: {len(cpp_files):,}\")\n",
    "    print(f\"‚úÖ Successfully processed: {len(processing_results):,}\")\n",
    "    print(f\"‚ùå Failed conversions: {len(cpp_files) - len(processing_results):,}\")\n",
    "    \n",
    "    if len(cpp_files) > 0:\n",
    "        success_rate = (len(processing_results) / len(cpp_files)) * 100\n",
    "        print(f\"üìä Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if 'ast_quality' in validation_results:\n",
    "        quality = validation_results['ast_quality']\n",
    "        if 'avg_sequence_length' in quality:\n",
    "            print(f\"üìè Average AST sequence length: {quality['avg_sequence_length']:.1f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Dataset ready for CodeBERT fine-tuning!\")\n",
    "    print(f\"üìã Check validation_report.json for detailed analysis\")\n",
    "    \n",
    "    return {\n",
    "        'dataset_path': output_dir,\n",
    "        'processing_results': processing_results,\n",
    "        'validation_results': validation_results,\n",
    "        'total_files': len(cpp_files)\n",
    "    }\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    results = main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plagdetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
