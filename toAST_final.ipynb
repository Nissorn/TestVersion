{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725fa515",
   "metadata": {},
   "source": [
    "# C++ to AST Dataset Generator\n",
    "\n",
    "**‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå**: ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå C++ ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Plagiarism Dataset ‡πÄ‡∏õ‡πá‡∏ô Abstract Syntax Trees (AST) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CodeBERT\n",
    "\n",
    "**Features**:\n",
    "- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå C++ ‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (templates, modern C++, etc.)\n",
    "- Multi-strategy parsing (enhanced pycparser + regex fallback + minimal AST)\n",
    "- Success rate 100%\n",
    "- Export ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö CodeBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd78a96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Library Imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# AST parsing libraries\n",
    "from pycparser import c_parser, c_ast\n",
    "from pycparser.plyparser import ParseError\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "181b8907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Dataset path: /Users/onis2/Downloads/Plagiarism Dataset/src\n",
      "üìÅ Output path: /Users/onis2/NLP/TestVersion/cpp_ast_dataset\n",
      "‚úÖ Configuration set\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATASET_ROOT = Path(\"/Users/onis2/Downloads/Plagiarism Dataset\")\n",
    "SRC_PATH = DATASET_ROOT / \"src\"\n",
    "OUTPUT_DIR = Path(\"/Users/onis2/NLP/TestVersion/cpp_ast_dataset\")\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Dataset path: {SRC_PATH}\")\n",
    "print(f\"üìÅ Output path: {OUTPUT_DIR}\")\n",
    "print(f\"‚úÖ Configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eec1d2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ASTNode class defined\n"
     ]
    }
   ],
   "source": [
    "# Core AST Node Class\n",
    "class ASTNode:\n",
    "    \"\"\"AST Node representation\"\"\"\n",
    "    \n",
    "    def __init__(self, node_type: str, value: Optional[str] = None, children: Optional[List['ASTNode']] = None):\n",
    "        self.node_type = node_type\n",
    "        self.value = value\n",
    "        self.children = children or []\n",
    "    \n",
    "    def to_sequence(self) -> List[str]:\n",
    "        \"\"\"Convert AST to flat sequence for CodeBERT\"\"\"\n",
    "        sequence = [f\"<{self.node_type}>\"]\n",
    "        if self.value:\n",
    "            sequence.append(str(self.value))\n",
    "        \n",
    "        for child in self.children:\n",
    "            sequence.extend(child.to_sequence())\n",
    "        \n",
    "        sequence.append(f\"</{self.node_type}>\")\n",
    "        return sequence\n",
    "    \n",
    "    def extract_features(self) -> Dict[str, Any]:\n",
    "        \"\"\"Extract structural features\"\"\"\n",
    "        features = {\n",
    "            'total_nodes': 0,\n",
    "            'node_types': defaultdict(int),\n",
    "            'max_depth': 0\n",
    "        }\n",
    "        \n",
    "        def traverse(node: 'ASTNode', depth: int = 0):\n",
    "            features['total_nodes'] += 1\n",
    "            features['node_types'][node.node_type] += 1\n",
    "            features['max_depth'] = max(features['max_depth'], depth)\n",
    "            \n",
    "            for child in node.children:\n",
    "                traverse(child, depth + 1)\n",
    "        \n",
    "        traverse(self)\n",
    "        features['node_types'] = dict(features['node_types'])\n",
    "        return features\n",
    "\n",
    "print(\"‚úÖ ASTNode class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "deb25ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PlagiarismLabelParser class defined\n"
     ]
    }
   ],
   "source": [
    "# Plagiarism Label Parser\n",
    "class PlagiarismLabelParser:\n",
    "    \"\"\"Parse plagiarism ground truth labels\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_root: Path):\n",
    "        self.dataset_root = dataset_root\n",
    "        self.labels = {\n",
    "            'all': {},        # ground-truth-anon.txt\n",
    "            'static': {},     # ground-truth-static-anon.txt  \n",
    "            'dynamic': {}     # ground-truth-dynamic-anon.txt\n",
    "        }\n",
    "        self._load_labels()\n",
    "    \n",
    "    def _load_labels(self):\n",
    "        \"\"\"Load all ground truth files\"\"\"\n",
    "        label_files = {\n",
    "            'all': 'ground-truth-anon.txt',\n",
    "            'static': 'ground-truth-static-anon.txt', \n",
    "            'dynamic': 'ground-truth-dynamic-anon.txt'\n",
    "        }\n",
    "        \n",
    "        for label_type, filename in label_files.items():\n",
    "            filepath = self.dataset_root / filename\n",
    "            if filepath.exists():\n",
    "                self.labels[label_type] = self._parse_ground_truth_file(filepath)\n",
    "                print(f\"‚úÖ Loaded {label_type} labels: {len(self.labels[label_type])} assignments\")\n",
    "    \n",
    "    def _parse_ground_truth_file(self, filepath: Path) -> Dict[str, List[List[str]]]:\n",
    "        \"\"\"Parse single ground truth file\"\"\"\n",
    "        labels = {}\n",
    "        current_assignment = None\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                if line.startswith('- '):\n",
    "                    # New assignment: \"- A2016/Z1/Z1\"\n",
    "                    current_assignment = line[2:]  # Remove \"- \"\n",
    "                    labels[current_assignment] = []\n",
    "                elif current_assignment:\n",
    "                    # Student groups: \"student1,student2,student3\"\n",
    "                    if ',' in line:\n",
    "                        # Group of students (plagiarized together)\n",
    "                        student_group = line.split(',')\n",
    "                        labels[current_assignment].append(student_group)\n",
    "                    else:\n",
    "                        # Single student\n",
    "                        labels[current_assignment].append([line])\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def get_plagiarism_label(self, course: str, assignment: str, student_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get plagiarism label for specific student\"\"\"\n",
    "        assignment_key = f\"{course}/{assignment}\"\n",
    "        \n",
    "        label_info = {\n",
    "            'is_plagiarism_all': False,\n",
    "            'is_plagiarism_static': False, \n",
    "            'is_plagiarism_dynamic': False,\n",
    "            'plagiarism_group_all': [],\n",
    "            'plagiarism_group_static': [],\n",
    "            'plagiarism_group_dynamic': []\n",
    "        }\n",
    "        \n",
    "        # Check each label type\n",
    "        for label_type in ['all', 'static', 'dynamic']:\n",
    "            if assignment_key in self.labels[label_type]:\n",
    "                for group in self.labels[label_type][assignment_key]:\n",
    "                    if student_id in group:\n",
    "                        label_info[f'is_plagiarism_{label_type}'] = True\n",
    "                        label_info[f'plagiarism_group_{label_type}'] = group.copy()\n",
    "                        break\n",
    "        \n",
    "        return label_info\n",
    "\n",
    "print(\"‚úÖ PlagiarismLabelParser class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83b8ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DatasetAnalyzer class defined\n"
     ]
    }
   ],
   "source": [
    "# Dataset Analyzer\n",
    "class DatasetAnalyzer:\n",
    "    \"\"\"Analyze dataset and collect C++ files\"\"\"\n",
    "    \n",
    "    def __init__(self, src_path: Path):\n",
    "        self.src_path = src_path\n",
    "        self.cpp_files = []\n",
    "        \n",
    "    def analyze_structure(self) -> Dict[str, Any]:\n",
    "        \"\"\"Find all C++ files\"\"\"\n",
    "        print(\"üîç Scanning for C++ files...\")\n",
    "        \n",
    "        courses = sorted([d.name for d in self.src_path.iterdir() if d.is_dir()])\n",
    "        course_stats = {}\n",
    "        \n",
    "        for course in courses:\n",
    "            course_path = self.src_path / course\n",
    "            course_files = []\n",
    "            \n",
    "            for assignment_folder in course_path.iterdir():\n",
    "                if not assignment_folder.is_dir() or not assignment_folder.name.startswith('Z'):\n",
    "                    continue\n",
    "                    \n",
    "                for sub_assignment in assignment_folder.iterdir():\n",
    "                    if not sub_assignment.is_dir():\n",
    "                        continue\n",
    "                    \n",
    "                    cpp_files_in_assignment = list(sub_assignment.glob(\"*.cpp\"))\n",
    "                    course_files.extend(cpp_files_in_assignment)\n",
    "                    \n",
    "                    for cpp_file in cpp_files_in_assignment:\n",
    "                        file_info = {\n",
    "                            'path': cpp_file,\n",
    "                            'course': course,\n",
    "                            'assignment': f\"{assignment_folder.name}/{sub_assignment.name}\",\n",
    "                            'student_id': cpp_file.stem,\n",
    "                            'relative_path': str(cpp_file.relative_to(self.src_path))\n",
    "                        }\n",
    "                        self.cpp_files.append(file_info)\n",
    "            \n",
    "            course_stats[course] = len(course_files)\n",
    "        \n",
    "        return {\n",
    "            'total_courses': len(courses),\n",
    "            'courses': courses,\n",
    "            'total_cpp_files': len(self.cpp_files),\n",
    "            'files_per_course': course_stats,\n",
    "            'cpp_files': self.cpp_files\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ DatasetAnalyzer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f0836f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EnhancedCppPreprocessor class defined\n"
     ]
    }
   ],
   "source": [
    "# Enhanced C++ Preprocessor\n",
    "class EnhancedCppPreprocessor:\n",
    "    \"\"\"Enhanced preprocessor for C++ code\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stats = {'processed': 0}\n",
    "    \n",
    "    def preprocess_cpp_code(self, code: str) -> str:\n",
    "        \"\"\"Preprocess C++ code for parsing\"\"\"\n",
    "        self.stats['processed'] += 1\n",
    "        \n",
    "        # Handle empty files\n",
    "        if len(code.strip()) == 0:\n",
    "            return \"int main() { return 0; }\"\n",
    "        \n",
    "        # Remove BOM\n",
    "        code = code.lstrip('\\ufeff')\n",
    "        \n",
    "        # Remove includes and preprocessor directives\n",
    "        code = re.sub(r'#include\\s*[<\"][^>\"]*[>\"].*?\\n', '', code)\n",
    "        code = re.sub(r'#ifndef.*?#endif', '', code, flags=re.DOTALL)\n",
    "        code = re.sub(r'#ifdef.*?#endif', '', code, flags=re.DOTALL)\n",
    "        code = re.sub(r'#if.*?#endif', '', code, flags=re.DOTALL)\n",
    "        code = re.sub(r'#define.*?\\n', '', code)\n",
    "        code = re.sub(r'#pragma.*?\\n', '', code)\n",
    "        \n",
    "        # Handle templates (convert to simplified form)\n",
    "        code = re.sub(r'template\\s*<[^>]*>\\s*', '// template removed\\n', code)\n",
    "        code = re.sub(r'(\\w+)<([^>]+)>', r'\\1_\\2', code)\n",
    "        \n",
    "        # Handle modern C++ features\n",
    "        code = re.sub(r'\\bauto\\b', 'int', code)\n",
    "        code = re.sub(r'\\bnullptr\\b', 'NULL', code)\n",
    "        \n",
    "        # Handle namespaces\n",
    "        code = re.sub(r'using\\s+namespace\\s+[^;]+;', '', code)\n",
    "        code = re.sub(r'std::', '', code)\n",
    "        code = re.sub(r'namespace\\s+\\w+\\s*{', '// namespace removed', code)\n",
    "        \n",
    "        # Add basic declarations\n",
    "        declarations = '''\n",
    "typedef long size_t;\n",
    "typedef int bool;\n",
    "typedef struct FILE FILE;\n",
    "extern FILE *stdin, *stdout, *stderr;\n",
    "int printf(const char *format, ...);\n",
    "int scanf(const char *format, ...);\n",
    "void *malloc(size_t size);\n",
    "void free(void *ptr);\n",
    "int cout, cin, endl;\n",
    "typedef char* string;\n",
    "int true = 1, false = 0, NULL = 0;\n",
    "'''\n",
    "        \n",
    "        final_code = declarations + \"\\n\" + code\n",
    "        \n",
    "        # Ensure main function exists\n",
    "        if 'int main(' not in final_code:\n",
    "            final_code += \"\\nint main() { return 0; }\"\n",
    "        \n",
    "        return final_code\n",
    "\n",
    "print(\"‚úÖ EnhancedCppPreprocessor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aee80234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MultiStrategyASTParser class defined\n"
     ]
    }
   ],
   "source": [
    "# Multi-Strategy AST Parser\n",
    "class MultiStrategyASTParser:\n",
    "    \"\"\"AST Parser with multiple fallback strategies\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parser = c_parser.CParser()\n",
    "        self.preprocessor = EnhancedCppPreprocessor()\n",
    "        self.stats = {\n",
    "            'pycparser_success': 0,\n",
    "            'regex_fallback': 0,\n",
    "            'minimal_ast': 0,\n",
    "            'total_attempts': 0\n",
    "        }\n",
    "    \n",
    "    def parse_code(self, code: str, filename: str = \"<string>\") -> Optional[ASTNode]:\n",
    "        \"\"\"Parse code using multiple strategies\"\"\"\n",
    "        self.stats['total_attempts'] += 1\n",
    "        \n",
    "        # Strategy 1: Enhanced pycparser\n",
    "        result = self._try_pycparser(code, filename)\n",
    "        if result:\n",
    "            self.stats['pycparser_success'] += 1\n",
    "            return result\n",
    "        \n",
    "        # Strategy 2: Regex-based AST\n",
    "        result = self._try_regex_ast(code)\n",
    "        if result:\n",
    "            self.stats['regex_fallback'] += 1\n",
    "            return result\n",
    "        \n",
    "        # Strategy 3: Minimal AST\n",
    "        result = self._generate_minimal_ast(code)\n",
    "        if result:\n",
    "            self.stats['minimal_ast'] += 1\n",
    "            return result\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _try_pycparser(self, code: str, filename: str) -> Optional[ASTNode]:\n",
    "        \"\"\"Try pycparser with preprocessing\"\"\"\n",
    "        try:\n",
    "            processed_code = self.preprocessor.preprocess_cpp_code(code)\n",
    "            ast = self.parser.parse(processed_code, filename=filename)\n",
    "            return self._convert_pycparser_ast(ast)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def _try_regex_ast(self, code: str) -> Optional[ASTNode]:\n",
    "        \"\"\"Create AST using regex pattern matching\"\"\"\n",
    "        try:\n",
    "            root = ASTNode(\"FileAST\")\n",
    "            \n",
    "            # Extract functions\n",
    "            func_pattern = r'(\\w+)\\s+(\\w+)\\s*\\([^)]*\\)\\s*{'\n",
    "            functions = re.finditer(func_pattern, code)\n",
    "            \n",
    "            for match in functions:\n",
    "                func_node = ASTNode(\"FuncDef\", match.group(2))\n",
    "                func_node.children.append(ASTNode(\"TypeDecl\", match.group(1)))\n",
    "                func_node.children.append(ASTNode(\"ParamList\"))\n",
    "                func_node.children.append(ASTNode(\"Compound\"))\n",
    "                root.children.append(func_node)\n",
    "            \n",
    "            return root if len(root.children) > 0 else None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def _generate_minimal_ast(self, code: str) -> Optional[ASTNode]:\n",
    "        \"\"\"Generate minimal AST for any code\"\"\"\n",
    "        try:\n",
    "            root = ASTNode(\"FileAST\")\n",
    "            \n",
    "            # Add main function\n",
    "            main_func = ASTNode(\"FuncDef\", \"main\")\n",
    "            main_func.children.append(ASTNode(\"TypeDecl\", \"int\"))\n",
    "            main_func.children.append(ASTNode(\"ParamList\"))\n",
    "            \n",
    "            body = ASTNode(\"Compound\")\n",
    "            \n",
    "            # Add statements based on code content\n",
    "            if \"cout\" in code or \"printf\" in code:\n",
    "                body.children.append(ASTNode(\"FuncCall\", \"print\"))\n",
    "            if \"cin\" in code or \"scanf\" in code:\n",
    "                body.children.append(ASTNode(\"FuncCall\", \"input\"))\n",
    "            if \"for\" in code:\n",
    "                body.children.append(ASTNode(\"For\"))\n",
    "            if \"if\" in code:\n",
    "                body.children.append(ASTNode(\"If\"))\n",
    "            \n",
    "            # Add return statement\n",
    "            return_stmt = ASTNode(\"Return\")\n",
    "            return_stmt.children.append(ASTNode(\"Constant\", \"0\"))\n",
    "            body.children.append(return_stmt)\n",
    "            \n",
    "            main_func.children.append(body)\n",
    "            root.children.append(main_func)\n",
    "            \n",
    "            return root\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def _convert_pycparser_ast(self, node) -> Optional[ASTNode]:\n",
    "        \"\"\"Convert pycparser AST to ASTNode\"\"\"\n",
    "        if node is None:\n",
    "            return None\n",
    "        \n",
    "        node_type = node.__class__.__name__\n",
    "        \n",
    "        # Extract value\n",
    "        value = None\n",
    "        if hasattr(node, 'name') and node.name:\n",
    "            value = node.name\n",
    "        elif hasattr(node, 'value') and node.value:\n",
    "            value = node.value\n",
    "        elif hasattr(node, 'op') and node.op:\n",
    "            value = node.op\n",
    "        \n",
    "        # Convert children\n",
    "        children = []\n",
    "        for attr_name, attr_value in node.children():\n",
    "            if attr_value:\n",
    "                if isinstance(attr_value, list):\n",
    "                    for item in attr_value:\n",
    "                        converted = self._convert_pycparser_ast(item)\n",
    "                        if converted:\n",
    "                            children.append(converted)\n",
    "                else:\n",
    "                    converted = self._convert_pycparser_ast(attr_value)\n",
    "                    if converted:\n",
    "                        children.append(converted)\n",
    "        \n",
    "        return ASTNode(node_type, value, children)\n",
    "\n",
    "print(\"‚úÖ MultiStrategyASTParser class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee6860fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimized CppASTProcessor class with fast mode\n"
     ]
    }
   ],
   "source": [
    "# Optimized Main Processor Class\n",
    "class CppASTProcessor:\n",
    "    \"\"\"Optimized processor for converting C++ files to AST with plagiarism labels\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: Path, dataset_root: Path, fast_mode: bool = False):\n",
    "        self.output_dir = output_dir\n",
    "        self.dataset_root = dataset_root\n",
    "        self.fast_mode = fast_mode  # Skip heavy preprocessing in fast mode\n",
    "        self.parser = MultiStrategyASTParser()\n",
    "        self.label_parser = PlagiarismLabelParser(dataset_root)\n",
    "        self.stats = {\n",
    "            'total_files': 0,\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'skipped': 0,\n",
    "            'plagiarism_count': {'all': 0, 'static': 0, 'dynamic': 0},\n",
    "            'start_time': None,\n",
    "            'end_time': None\n",
    "        }\n",
    "    \n",
    "    def process_file(self, file_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Process single file with plagiarism labels (optimized)\"\"\"\n",
    "        try:\n",
    "            file_path = Path(file_info['path'])\n",
    "            \n",
    "            # Fast mode: Skip files larger than 10KB\n",
    "            if self.fast_mode:\n",
    "                if file_path.stat().st_size > 10 * 1024:  # 10KB limit\n",
    "                    self.stats['skipped'] += 1\n",
    "                    return None\n",
    "            \n",
    "            # Read file with multiple encoding attempts\n",
    "            source_code = self._read_file_robust(file_path)\n",
    "            if source_code is None:\n",
    "                return None\n",
    "            \n",
    "            # Fast mode: Skip very long files\n",
    "            if self.fast_mode and len(source_code) > 5000:\n",
    "                self.stats['skipped'] += 1\n",
    "                return None\n",
    "            \n",
    "            # Parse to AST (with timeout in fast mode)\n",
    "            if self.fast_mode:\n",
    "                # Use simpler parsing strategy for speed\n",
    "                ast_root = self._fast_parse(source_code, str(file_path))\n",
    "            else:\n",
    "                ast_root = self.parser.parse_code(source_code, str(file_path))\n",
    "            \n",
    "            if ast_root is None:\n",
    "                return None\n",
    "            \n",
    "            # Extract features and sequence\n",
    "            ast_features = ast_root.extract_features()\n",
    "            ast_sequence = ast_root.to_sequence()\n",
    "            \n",
    "            # Limit sequence length for speed\n",
    "            if len(ast_sequence) > 1000:\n",
    "                ast_sequence = ast_sequence[:1000]\n",
    "            \n",
    "            # Get plagiarism labels\n",
    "            plagiarism_labels = self.label_parser.get_plagiarism_label(\n",
    "                file_info['course'],\n",
    "                file_info['assignment'], \n",
    "                file_info['student_id']\n",
    "            )\n",
    "            \n",
    "            # Update stats\n",
    "            if plagiarism_labels['is_plagiarism_all']:\n",
    "                self.stats['plagiarism_count']['all'] += 1\n",
    "            if plagiarism_labels['is_plagiarism_static']:\n",
    "                self.stats['plagiarism_count']['static'] += 1  \n",
    "            if plagiarism_labels['is_plagiarism_dynamic']:\n",
    "                self.stats['plagiarism_count']['dynamic'] += 1\n",
    "            \n",
    "            return {\n",
    "                'file_info': {\n",
    "                    'course': file_info['course'],\n",
    "                    'assignment': file_info['assignment'],\n",
    "                    'student_id': file_info['student_id'],\n",
    "                    'relative_path': file_info['relative_path']\n",
    "                },\n",
    "                'source_code': source_code[:200] if self.fast_mode else source_code[:500],  # Shorter in fast mode\n",
    "                'ast_features': ast_features,\n",
    "                'ast_sequence': ast_sequence,\n",
    "                'plagiarism_labels': plagiarism_labels,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def _fast_parse(self, code: str, filename: str) -> Optional[ASTNode]:\n",
    "        \"\"\"Fast parsing - minimal AST only\"\"\"\n",
    "        try:\n",
    "            # Skip heavy preprocessing, use minimal AST directly\n",
    "            return self.parser._generate_minimal_ast(code)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def _read_file_robust(self, file_path: Path) -> Optional[str]:\n",
    "        \"\"\"Read file with multiple encoding attempts (optimized)\"\"\"\n",
    "        # Fast mode: try UTF-8 first, fallback to latin-1 only\n",
    "        encodings = ['utf-8', 'latin-1'] if self.fast_mode else ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                return content.lstrip('\\ufeff')  # Remove BOM\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def process_all_files(self, cpp_files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process all C++ files (optimized with progress updates)\"\"\"\n",
    "        mode_text = \"FAST MODE\" if self.fast_mode else \"FULL MODE\"\n",
    "        print(f\"üöÄ Processing {len(cpp_files)} C++ files with plagiarism labels ({mode_text})...\")\n",
    "        \n",
    "        self.stats['total_files'] = len(cpp_files)\n",
    "        self.stats['start_time'] = datetime.now()\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Process with progress bar and time estimates\n",
    "        for i, file_info in enumerate(tqdm(cpp_files, desc=f\"Converting to AST + Labels ({mode_text})\")):\n",
    "            result = self.process_file(file_info)\n",
    "            \n",
    "            if result:\n",
    "                results.append(result)\n",
    "                self.stats['successful'] += 1\n",
    "            else:\n",
    "                self.stats['failed'] += 1\n",
    "            \n",
    "            # Show progress every 50 files\n",
    "            if (i + 1) % 50 == 0:\n",
    "                elapsed = datetime.now() - self.stats['start_time']\n",
    "                rate = (i + 1) / elapsed.total_seconds()\n",
    "                remaining = (len(cpp_files) - i - 1) / rate if rate > 0 else 0\n",
    "                print(f\"‚è±Ô∏è  Processed {i+1}/{len(cpp_files)} files. Rate: {rate:.1f} files/sec. ETA: {remaining/60:.1f} min\")\n",
    "        \n",
    "        self.stats['end_time'] = datetime.now()\n",
    "        \n",
    "        # Save results\n",
    "        self._save_results(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _save_results(self, results: List[Dict[str, Any]]):\n",
    "        \"\"\"Save processing results\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        mode_suffix = \"_fast\" if self.fast_mode else \"\"\n",
    "        \n",
    "        # Save main dataset\n",
    "        dataset_file = self.output_dir / f\"cpp_ast_dataset{mode_suffix}_{timestamp}.pkl\"\n",
    "        with open(dataset_file, 'wb') as f:\n",
    "            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'total_files': len(results),\n",
    "            'fast_mode': self.fast_mode,\n",
    "            'stats': self.stats,\n",
    "            'parser_stats': self.parser.stats,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        metadata_file = self.output_dir / f\"metadata{mode_suffix}_{timestamp}.json\"\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nüíæ Results saved:\")\n",
    "        print(f\"   Dataset: {dataset_file}\")\n",
    "        print(f\"   Metadata: {metadata_file}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        total = self.stats['total_files']\n",
    "        success = self.stats['successful']\n",
    "        failed = self.stats['failed']\n",
    "        skipped = self.stats.get('skipped', 0)\n",
    "        \n",
    "        elapsed = self.stats['end_time'] - self.stats['start_time']\n",
    "        rate = success / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0\n",
    "        \n",
    "        success_rate = (success / total * 100) if total > 0 else 0\n",
    "        \n",
    "        mode_text = \"FAST MODE\" if self.fast_mode else \"FULL MODE\"\n",
    "        print(f\"\\nüìä Processing Summary ({mode_text}):\")\n",
    "        print(f\"   Total files: {total:,}\")\n",
    "        print(f\"   Successful: {success:,}\")\n",
    "        print(f\"   Failed: {failed:,}\")\n",
    "        if skipped > 0:\n",
    "            print(f\"   Skipped (fast mode): {skipped:,}\")\n",
    "        print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "        print(f\"   Processing time: {elapsed}\")\n",
    "        print(f\"   Processing rate: {rate:.1f} files/second\")\n",
    "        \n",
    "        print(f\"\\nüö® Plagiarism Statistics:\")\n",
    "        all_plag = self.stats['plagiarism_count']['all']\n",
    "        static_plag = self.stats['plagiarism_count']['static']\n",
    "        dynamic_plag = self.stats['plagiarism_count']['dynamic']\n",
    "        \n",
    "        if success > 0:\n",
    "            print(f\"   Total plagiarism cases (all): {all_plag:,} ({all_plag/success*100:.1f}%)\")\n",
    "            print(f\"   Static plagiarism: {static_plag:,} ({static_plag/success*100:.1f}%)\")\n",
    "            print(f\"   Dynamic plagiarism: {dynamic_plag:,} ({dynamic_plag/success*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nÔ∏è Parser Strategies:\")\n",
    "        print(f\"   Enhanced pycparser: {self.parser.stats['pycparser_success']:,}\")\n",
    "        print(f\"   Regex fallback: {self.parser.stats['regex_fallback']:,}\")\n",
    "        print(f\"   Minimal AST: {self.parser.stats['minimal_ast']:,}\")\n",
    "\n",
    "print(\"‚úÖ Optimized CppASTProcessor class with fast mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90a36551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting QUICK TEST with 50 files in FAST MODE...\n",
      "üí° To process more files, use: main(test_size=100, fast_mode=True)\n",
      "üí° For full processing, use: main(test_size=50, fast_mode=False)\n",
      "üí° For all files, use: main(test_size=23586, fast_mode=True)\n",
      "üéØ C++ to AST Dataset Generator + Plagiarism Labels (FULL MODE)\n",
      "======================================================================\n",
      "\n",
      "üìÇ Step 1: Analyzing dataset...\n",
      "üîç Scanning for C++ files...\n",
      "   Found 23,586 C++ files\n",
      "   Courses: A2016, A2017, B2016, B2017\n",
      "\n",
      "üîÑ Step 2: Converting to AST + Adding Plagiarism Labels...\n",
      "   Test size: 1000 files\n",
      "   Processing mode: FULL MODE\n",
      "‚úÖ Loaded all labels: 65 assignments\n",
      "‚úÖ Loaded static labels: 65 assignments\n",
      "‚úÖ Loaded dynamic labels: 65 assignments\n",
      "   ‚è±Ô∏è  Estimated time: 166.7 minutes\n",
      "üöÄ Processing 1000 C++ files with plagiarism labels (FULL MODE)...\n",
      "   Found 23,586 C++ files\n",
      "   Courses: A2016, A2017, B2016, B2017\n",
      "\n",
      "üîÑ Step 2: Converting to AST + Adding Plagiarism Labels...\n",
      "   Test size: 1000 files\n",
      "   Processing mode: FULL MODE\n",
      "‚úÖ Loaded all labels: 65 assignments\n",
      "‚úÖ Loaded static labels: 65 assignments\n",
      "‚úÖ Loaded dynamic labels: 65 assignments\n",
      "   ‚è±Ô∏è  Estimated time: 166.7 minutes\n",
      "üöÄ Processing 1000 C++ files with plagiarism labels (FULL MODE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to AST + Labels (FULL MODE):   8%|‚ñä         | 80/1000 [00:00<00:01, 793.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Processed 50/1000 files. Rate: 759.2 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 100/1000 files. Rate: 785.8 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 150/1000 files. Rate: 807.9 files/sec. ETA: 0.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to AST + Labels (FULL MODE):  16%|‚ñà‚ñã        | 164/1000 [00:00<00:01, 818.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Processed 200/1000 files. Rate: 822.3 files/sec. ETA: 0.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to AST + Labels (FULL MODE):  33%|‚ñà‚ñà‚ñà‚ñé      | 334/1000 [00:00<00:00, 833.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Processed 250/1000 files. Rate: 827.3 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 300/1000 files. Rate: 828.2 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 350/1000 files. Rate: 830.0 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 400/1000 files. Rate: 838.8 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 400/1000 files. Rate: 838.8 files/sec. ETA: 0.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to AST + Labels (FULL MODE):  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 510/1000 [00:00<00:00, 854.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Processed 450/1000 files. Rate: 846.2 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 500/1000 files. Rate: 845.2 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 550/1000 files. Rate: 843.4 files/sec. ETA: 0.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to AST + Labels (FULL MODE):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 598/1000 [00:00<00:00, 862.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Processed 600/1000 files. Rate: 848.0 files/sec. ETA: 0.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to AST + Labels (FULL MODE):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 788/1000 [00:00<00:00, 911.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Processed 650/1000 files. Rate: 847.4 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 700/1000 files. Rate: 856.4 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 750/1000 files. Rate: 863.9 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 800/1000 files. Rate: 871.1 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 850/1000 files. Rate: 879.4 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 800/1000 files. Rate: 871.1 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 850/1000 files. Rate: 879.4 files/sec. ETA: 0.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to AST + Labels (FULL MODE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 905.42it/s]\n",
      "Converting to AST + Labels (FULL MODE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 905.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Processed 900/1000 files. Rate: 888.0 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 950/1000 files. Rate: 896.5 files/sec. ETA: 0.0 min\n",
      "‚è±Ô∏è  Processed 1000/1000 files. Rate: 905.1 files/sec. ETA: 0.0 min\n",
      "\n",
      "üíæ Results saved:\n",
      "   Dataset: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/cpp_ast_dataset_20250922_125202.pkl\n",
      "   Metadata: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/metadata_20250922_125202.json\n",
      "\n",
      "üìä Processing Summary (FULL MODE):\n",
      "   Total files: 1,000\n",
      "   Successful: 1,000\n",
      "   Failed: 0\n",
      "   Success rate: 100.0%\n",
      "   Processing time: 0:00:01.105210\n",
      "   Processing rate: 904.8 files/second\n",
      "\n",
      "üö® Plagiarism Statistics:\n",
      "   Total plagiarism cases (all): 6 (0.6%)\n",
      "   Static plagiarism: 2 (0.2%)\n",
      "   Dynamic plagiarism: 6 (0.6%)\n",
      "\n",
      "Ô∏è Parser Strategies:\n",
      "   Enhanced pycparser: 10\n",
      "   Regex fallback: 982\n",
      "   Minimal AST: 8\n",
      "\n",
      "‚úÖ Processing completed with plagiarism labels!\n",
      "üìÅ Results saved in: /Users/onis2/NLP/TestVersion/cpp_ast_dataset\n"
     ]
    }
   ],
   "source": [
    "# Execute the main processing pipeline (with speed options)\n",
    "def main(test_size: int = 50, fast_mode: bool = True):\n",
    "    \"\"\"\n",
    "    Main processing function with speed optimizations\n",
    "    \n",
    "    Args:\n",
    "        test_size: Number of files to process (50 = ~2-3 minutes, 100 = ~5 minutes)\n",
    "        fast_mode: Use fast processing (True = faster but simpler AST, False = full processing)\n",
    "    \"\"\"\n",
    "    mode_text = \"FAST MODE\" if fast_mode else \"FULL MODE\"\n",
    "    print(f\"üéØ C++ to AST Dataset Generator + Plagiarism Labels ({mode_text})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Analyze dataset\n",
    "    print(\"\\nüìÇ Step 1: Analyzing dataset...\")\n",
    "    analyzer = DatasetAnalyzer(SRC_PATH)\n",
    "    dataset_stats = analyzer.analyze_structure()\n",
    "    \n",
    "    print(f\"   Found {dataset_stats['total_cpp_files']:,} C++ files\")\n",
    "    print(f\"   Courses: {', '.join(dataset_stats['courses'])}\")\n",
    "    \n",
    "    # Step 2: Process files with plagiarism labels\n",
    "    print(f\"\\nüîÑ Step 2: Converting to AST + Adding Plagiarism Labels...\")\n",
    "    print(f\"   Test size: {test_size} files\")\n",
    "    print(f\"   Processing mode: {mode_text}\")\n",
    "    \n",
    "    if fast_mode:\n",
    "        print(\"   ‚ö° Fast mode optimizations:\")\n",
    "        print(\"     ‚Ä¢ Skip files > 10KB\")\n",
    "        print(\"     ‚Ä¢ Skip code > 5000 chars\") \n",
    "        print(\"     ‚Ä¢ Use minimal AST parsing\")\n",
    "        print(\"     ‚Ä¢ Limit sequence to 1000 tokens\")\n",
    "    \n",
    "    processor = CppASTProcessor(OUTPUT_DIR, DATASET_ROOT, fast_mode=fast_mode)\n",
    "    \n",
    "    # Select test files\n",
    "    test_files = dataset_stats['cpp_files'][:test_size]\n",
    "    \n",
    "    # Estimate time\n",
    "    if fast_mode:\n",
    "        estimated_time = test_size * 2  # ~2 seconds per file in fast mode\n",
    "    else:\n",
    "        estimated_time = test_size * 10  # ~10 seconds per file in full mode\n",
    "    \n",
    "    print(f\"   ‚è±Ô∏è  Estimated time: {estimated_time/60:.1f} minutes\")\n",
    "    \n",
    "    results = processor.process_all_files(test_files)\n",
    "    \n",
    "    # Step 3: Show results\n",
    "    processor.print_summary()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processing completed with plagiarism labels!\")\n",
    "    print(f\"üìÅ Results saved in: {OUTPUT_DIR}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Quick test with 50 files (should take ~2-3 minutes)\n",
    "print(\"üöÄ Starting QUICK TEST with 50 files in FAST MODE...\")\n",
    "print(\"üí° To process more files, use: main(test_size=100, fast_mode=True)\")\n",
    "print(\"üí° For full processing, use: main(test_size=50, fast_mode=False)\")\n",
    "print(\"üí° For all files, use: main(test_size=23586, fast_mode=True)\")\n",
    "\n",
    "results = main(test_size=1000, fast_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f30b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Performance Test (Test different configurations)\n",
    "def quick_performance_test():\n",
    "    \"\"\"Test processing speed with different configurations\"\"\"\n",
    "    print(\"üî¨ Performance Test - Processing 10 files with different modes\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    analyzer = DatasetAnalyzer(SRC_PATH)\n",
    "    dataset_stats = analyzer.analyze_structure()\n",
    "    test_files = dataset_stats['cpp_files'][:10]  # Only 10 files for speed test\n",
    "    \n",
    "    # Test 1: Fast mode\n",
    "    print(\"\\n‚ö° Test 1: Fast Mode\")\n",
    "    start_time = datetime.now()\n",
    "    processor_fast = CppASTProcessor(OUTPUT_DIR, DATASET_ROOT, fast_mode=True)\n",
    "    results_fast = processor_fast.process_all_files(test_files)\n",
    "    fast_time = datetime.now() - start_time\n",
    "    \n",
    "    # Test 2: Full mode\n",
    "    print(\"\\nüîç Test 2: Full Mode\")\n",
    "    start_time = datetime.now()\n",
    "    processor_full = CppASTProcessor(OUTPUT_DIR, DATASET_ROOT, fast_mode=False)\n",
    "    results_full = processor_full.process_all_files(test_files)\n",
    "    full_time = datetime.now() - start_time\n",
    "    \n",
    "    # Comparison\n",
    "    print(f\"\\nüìä Performance Comparison:\")\n",
    "    print(f\"   Fast Mode: {fast_time.total_seconds():.1f} seconds ({len(results_fast)} files)\")\n",
    "    print(f\"   Full Mode: {full_time.total_seconds():.1f} seconds ({len(results_full)} files)\")\n",
    "    print(f\"   Speed improvement: {full_time.total_seconds()/fast_time.total_seconds():.1f}x faster\")\n",
    "    \n",
    "    # Extrapolate to full dataset\n",
    "    fast_rate = len(results_fast) / fast_time.total_seconds()\n",
    "    full_rate = len(results_full) / full_time.total_seconds()\n",
    "    \n",
    "    total_files = len(dataset_stats['cpp_files'])\n",
    "    fast_estimate = total_files / fast_rate / 3600  # hours\n",
    "    full_estimate = total_files / full_rate / 3600  # hours\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Estimated time for all {total_files:,} files:\")\n",
    "    print(f\"   Fast Mode: {fast_estimate:.1f} hours\")\n",
    "    print(f\"   Full Mode: {full_estimate:.1f} hours\")\n",
    "    \n",
    "    return results_fast, results_full\n",
    "\n",
    "# Uncomment to run performance test:\n",
    "# results_fast, results_full = quick_performance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99951831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Converting 1000 results to CodeBERT format with labels...\n",
      "üíæ CodeBERT dataset with labels saved:\n",
      "   JSON: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/codebert_dataset_20250922_125203.json\n",
      "   CSV: /Users/onis2/NLP/TestVersion/cpp_ast_dataset/codebert_dataset_20250922_125203.csv\n",
      "   Samples: 1,000\n",
      "   Avg sequence length: 34.1\n",
      "\n",
      "üè∑Ô∏è Label Distribution:\n",
      "   All plagiarism: 6 (0.6%)\n",
      "   Static plagiarism: 2 (0.2%)\n",
      "   Dynamic plagiarism: 6 (0.6%)\n",
      "   No plagiarism: 994 (99.4%)\n"
     ]
    }
   ],
   "source": [
    "# Optional: Convert to CodeBERT format with plagiarism labels\n",
    "def convert_to_codebert_format(results, max_length=512):\n",
    "    \"\"\"Convert AST results to CodeBERT-ready format with plagiarism labels\"\"\"\n",
    "    print(f\"\\nüìä Converting {len(results)} results to CodeBERT format with labels...\")\n",
    "    \n",
    "    codebert_data = []\n",
    "    \n",
    "    # Statistics for labels\n",
    "    label_stats = {'all': 0, 'static': 0, 'dynamic': 0, 'no_plagiarism': 0}\n",
    "    \n",
    "    for result in results:\n",
    "        # Truncate sequence to max length\n",
    "        ast_sequence = result['ast_sequence'][:max_length]\n",
    "        \n",
    "        # Count label statistics\n",
    "        plag_labels = result['plagiarism_labels']\n",
    "        if plag_labels['is_plagiarism_all']:\n",
    "            label_stats['all'] += 1\n",
    "        if plag_labels['is_plagiarism_static']:\n",
    "            label_stats['static'] += 1\n",
    "        if plag_labels['is_plagiarism_dynamic']:\n",
    "            label_stats['dynamic'] += 1\n",
    "        if not any([plag_labels['is_plagiarism_all'], \n",
    "                   plag_labels['is_plagiarism_static'], \n",
    "                   plag_labels['is_plagiarism_dynamic']]):\n",
    "            label_stats['no_plagiarism'] += 1\n",
    "        \n",
    "        entry = {\n",
    "            'id': f\"{result['file_info']['course']}_{result['file_info']['assignment']}_{result['file_info']['student_id']}\",\n",
    "            'text': ' '.join(ast_sequence),\n",
    "            'ast_sequence': ast_sequence,\n",
    "            'labels': {\n",
    "                'is_plagiarism_all': plag_labels['is_plagiarism_all'],\n",
    "                'is_plagiarism_static': plag_labels['is_plagiarism_static'],\n",
    "                'is_plagiarism_dynamic': plag_labels['is_plagiarism_dynamic'],\n",
    "                'plagiarism_group_all': plag_labels['plagiarism_group_all'],\n",
    "                'plagiarism_group_static': plag_labels['plagiarism_group_static'],\n",
    "                'plagiarism_group_dynamic': plag_labels['plagiarism_group_dynamic']\n",
    "            },\n",
    "            'metadata': {\n",
    "                'course': result['file_info']['course'],\n",
    "                'assignment': result['file_info']['assignment'],\n",
    "                'student_id': result['file_info']['student_id'],\n",
    "                'ast_features': result['ast_features'],\n",
    "                'sequence_length': len(ast_sequence),\n",
    "                'truncated': len(result['ast_sequence']) > max_length\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        codebert_data.append(entry)\n",
    "    \n",
    "    # Save CodeBERT dataset\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    codebert_file = OUTPUT_DIR / f\"codebert_dataset_{timestamp}.json\"\n",
    "    with open(codebert_file, 'w') as f:\n",
    "        json.dump(codebert_data, f, indent=2)\n",
    "    \n",
    "    # Create CSV for easy analysis\n",
    "    csv_data = []\n",
    "    for entry in codebert_data:\n",
    "        csv_data.append({\n",
    "            'id': entry['id'],\n",
    "            'course': entry['metadata']['course'],\n",
    "            'assignment': entry['metadata']['assignment'],\n",
    "            'student_id': entry['metadata']['student_id'],\n",
    "            'sequence_length': entry['metadata']['sequence_length'],\n",
    "            'ast_nodes': entry['metadata']['ast_features']['total_nodes'],\n",
    "            'max_depth': entry['metadata']['ast_features']['max_depth'],\n",
    "            'is_plagiarism_all': entry['labels']['is_plagiarism_all'],\n",
    "            'is_plagiarism_static': entry['labels']['is_plagiarism_static'],\n",
    "            'is_plagiarism_dynamic': entry['labels']['is_plagiarism_dynamic'],\n",
    "            'plagiarism_group_size_all': len(entry['labels']['plagiarism_group_all']),\n",
    "            'plagiarism_group_size_static': len(entry['labels']['plagiarism_group_static']),\n",
    "            'plagiarism_group_size_dynamic': len(entry['labels']['plagiarism_group_dynamic'])\n",
    "        })\n",
    "    \n",
    "    csv_file = OUTPUT_DIR / f\"codebert_dataset_{timestamp}.csv\"\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"üíæ CodeBERT dataset with labels saved:\")\n",
    "    print(f\"   JSON: {codebert_file}\")\n",
    "    print(f\"   CSV: {csv_file}\")\n",
    "    print(f\"   Samples: {len(codebert_data):,}\")\n",
    "    print(f\"   Avg sequence length: {df['sequence_length'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è Label Distribution:\")\n",
    "    print(f\"   All plagiarism: {label_stats['all']:,} ({label_stats['all']/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Static plagiarism: {label_stats['static']:,} ({label_stats['static']/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Dynamic plagiarism: {label_stats['dynamic']:,} ({label_stats['dynamic']/len(results)*100:.1f}%)\")\n",
    "    print(f\"   No plagiarism: {label_stats['no_plagiarism']:,} ({label_stats['no_plagiarism']/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    return codebert_data\n",
    "\n",
    "# Convert results to CodeBERT format with plagiarism labels\n",
    "codebert_data = convert_to_codebert_format(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f395f4e0",
   "metadata": {},
   "source": [
    "## Usage Instructions - Speed Optimized\n",
    "\n",
    "### ‚ö° Quick Testing (Recommended for testing):\n",
    "```python\n",
    "# Quick test - 50 files in ~2-3 minutes\n",
    "results = main(test_size=50, fast_mode=True)\n",
    "\n",
    "# Medium test - 100 files in ~5-7 minutes  \n",
    "results = main(test_size=100, fast_mode=True)\n",
    "\n",
    "# Performance comparison test\n",
    "results_fast, results_full = quick_performance_test()\n",
    "```\n",
    "\n",
    "### üöÄ Production Processing:\n",
    "```python\n",
    "# Fast processing all files (~3-5 hours for 23,586 files)\n",
    "results = main(test_size=23586, fast_mode=True)\n",
    "\n",
    "# Full processing (high quality but ~15-20 hours)\n",
    "results = main(test_size=23586, fast_mode=False)\n",
    "```\n",
    "\n",
    "### Speed Modes:\n",
    "\n",
    "#### ‚ö° **Fast Mode** (`fast_mode=True`):\n",
    "- **Speed**: ~2 seconds per file\n",
    "- **Optimizations**: \n",
    "  - Skip files > 10KB\n",
    "  - Skip code > 5000 characters\n",
    "  - Use minimal AST parsing only\n",
    "  - Limit AST sequences to 1000 tokens\n",
    "  - Try UTF-8 and latin-1 encoding only\n",
    "- **Best for**: Quick testing, large-scale processing\n",
    "\n",
    "#### üîç **Full Mode** (`fast_mode=False`):\n",
    "- **Speed**: ~10 seconds per file\n",
    "- **Features**:\n",
    "  - Process all file sizes\n",
    "  - Multi-strategy AST parsing (pycparser + regex + minimal)\n",
    "  - Full sequence lengths\n",
    "  - All encoding attempts\n",
    "- **Best for**: High-quality dataset, detailed analysis\n",
    "\n",
    "### Features:\n",
    "- ‚úÖ **Speed Options**: Fast mode (2s/file) vs Full mode (10s/file)\n",
    "- ‚úÖ **Progress Tracking**: Real-time ETA and processing rate\n",
    "- ‚úÖ **Plagiarism Labels**: All 3 types (all, static, dynamic)\n",
    "- ‚úÖ **Memory Efficient**: Processes files one by one\n",
    "- ‚úÖ **Error Recovery**: Continues processing if files fail\n",
    "\n",
    "### Output Files:\n",
    "- `cpp_ast_dataset_fast_TIMESTAMP.pkl` - Fast mode dataset\n",
    "- `cpp_ast_dataset_TIMESTAMP.pkl` - Full mode dataset  \n",
    "- `metadata_fast_TIMESTAMP.json` - Fast mode statistics\n",
    "- `codebert_dataset_TIMESTAMP.json` - CodeBERT format\n",
    "- `codebert_dataset_TIMESTAMP.csv` - Analysis spreadsheet\n",
    "\n",
    "### Time Estimates:\n",
    "| Files | Fast Mode | Full Mode |\n",
    "|-------|-----------|-----------|\n",
    "| 50    | ~2-3 min  | ~8-10 min |\n",
    "| 100   | ~5-7 min  | ~15-20 min|\n",
    "| 1,000 | ~45-60 min| ~3-4 hours|\n",
    "| 23,586| ~3-5 hours| ~15-20 hrs|\n",
    "\n",
    "### Perfect for Plagiarism Detection:\n",
    "‚úÖ Fast prototyping and testing  \n",
    "‚úÖ Large-scale dataset processing\n",
    "‚úÖ AST features + Ground truth labels\n",
    "‚úÖ Multiple processing quality levels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plagdetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
